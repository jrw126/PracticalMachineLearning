---
title: "Practical Machine Learning Project"
author: "John Wright"
date: "Sunday, November 09, 2014"
output: html_document
---

# Load data and libraries
```{r}
library(caret)
library(corrgram)
library(Hmisc)
library(randomForest)
library(ggplot2)

train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```

## Data Documentation
http://groupware.les.inf.puc-rio.br/har

# Data Cleaning
Taking a quick visual glance at the train and test sets, it looks like there are many variables in the training set where most of the values are missing and several variables in the test set where _ALL_ of the values are missing. It doesn't make much sense to use a variable to train a model if it won't have any corresponding values in the testing set, so these columns should be identified and removed.

Additionally, since we want this model to be generalizable, it doesn't make sense to use the participant's name as a feature. Perhaps not everyone who's weightlifting performance want to predict is named `r unique(train$user_name)`. Similar logic can be applied to the timestamps, the variable `X` which appears to be a row number, and the `new_window` and `num_window` variables which also appear to be metadata.
```{r}
# Column indices of columns that aren't all NA. Also ignores the first 5 columns.
idx <- setdiff(which(!sapply(1:ncol(test), function(x) sum(is.na(test[, x])) == nrow(test))), 1:7)

train <- train[, idx]
test <- test[, idx]

# Check if there are any mismatched data types.
t <- which(!sapply(train, class) == sapply(test, class))
t

# Convert the types that don't match
test[, t[1:3]] <- lapply(test[, t[1:3]], as.numeric)
test$problem_id <- factor(test$problem_id)

# Split the training and testing sets
inTrain <- createDataPartition(train$classe, p = 3/4, list = FALSE)
training <- train[inTrain, ]
testing <- train[-inTrain, ]

qplot(1:nrow(train), train$pitch_belt, color = train$classe)
```

# Exploratory Analysis
Often times, data sets will contain variables that are highly correlated and may contain the same or similar information. To investigate if this scenario is occuring with this data, some correlation analysis is performed. First, a correlogram is generated to visually identify if there are any highly correlated variables. Darkly shaded boxes confirm the presence of variables that are higly correlated with each other.
```{r}
# Generate correlogram
corrgram(training, lower.panel = panel.shade, upper.panel = NULL, text.panel = panel.txt, main = "Correlogram of Numeric Variables")

# Identify the correlated data. Non-numeric columns are ignored.
M <- abs(cor(training[, sapply(training, is.numeric)]))
diag(M) <- 0 # Ignore correlation of variables with itself.
M <- which(M > 0.8, arr.ind = T)

# Return the amount of highly correlated numeric variables
nrow(M)
```

# Model Selection
The first model is a K nearest neighbors model using the numeric data from the training set after it has been preprocessed using principal components analysis.
```{r}
# Set seed
set.seed(123)

# Perform PCA preprocessing on all numeric variables
pp <- preProcess(training[, sapply(training, is.numeric)], method = "pca")
m1_PC <- predict(pp, training[, sapply(training, is.numeric)])

# Fit a model using K nearest neighbors on the preprocessed data.
m1_fit <- train(training$classe ~ ., method = "knn", data = m1_PC)

# Predict the test data values
m1_pred <- predict(pp, testing[, sapply(testing, is.numeric)])
problem_id <- predict(m1_fit, m1_pred)

# Evaluate the model
confusionMatrix(problem_id, testing$classe)
```

The second model is a random forest using all variables.
```{r}
# Fit the model
m2_fit <- randomForest(classe ~ ., data = training)

# Predict the test data
m2_pred <- predict(m2_fit, testing)

# Evaluate the model
confusionMatrix(m2_pred, testing$classe)

plot(m2_fit)
```

Perhaps unsurprisingly, the random forest model that does not ignore the factor variables like the first model that used PCA performs slightly better. The random forest model will be used to classify the data in the final test set.

See which variables are the most important
```{r}
data.frame(names(training)[order(-varImp(m2_fit))])
```

